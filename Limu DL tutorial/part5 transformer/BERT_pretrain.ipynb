{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Representation\n",
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"Get tokens of the BERT input sequence and their segment IDs\"\"\"   \n",
    "    # 添加特殊标记，并连接第一个句子的标记\n",
    "    tokens = ['<cls>'] + tokens_a + ['<seq>']\n",
    "    # 第一个句子的段ID都为0\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        # 如果存在第二个句子，则连接第二个句子的标记\n",
    "        tokens += tokens_b + ['<seq>']\n",
    "        # 第二个句子的段ID都为1\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    # 返回转换后的标记列表和段ID列表\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTEncoder class\n",
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT encoder.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        # 标记嵌入层\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        # 段嵌入层\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        # BERT编码器块的序列容器\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            # 添加BERT编码器块\n",
    "            self.blks.add_module(f\"{i}\", d2l.EncoderBlock(\n",
    "                key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # 位置嵌入参数\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))   \n",
    "    \n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # 计算输入序列的嵌入表示\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)  \n",
    "        # 添加位置嵌入\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            # 通过BERT编码器块进行编码\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT encoder.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        # 标记嵌入层\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        # 段嵌入层\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        # BERT编码器块的序列容器\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            # 添加BERT编码器块\n",
    "            self.blks.add_module(f\"{i}\", d2l.EncoderBlock(\n",
    "                key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # 位置嵌入参数\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
    "                                                      num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # 计算嵌入表示\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        # 添加位置嵌入\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            # 进行编码\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference of BERTEncoder\n",
    "# 定义BERT编码器的参数\n",
    "# vocab_size: 词汇表大小\n",
    "# num_hiddens: 隐藏单元数\n",
    "# ffn_num_hiddens: 前馈神经网络隐藏层大小\n",
    "# num_heads: 注意力头数\n",
    "# norm_shape: 规范化层的形状\n",
    "# ffn_num_input: 前馈神经网络输入大小\n",
    "# num_layers: 编码器层数\n",
    "# dropout: dropout概率\n",
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n",
    "# 创建BERT编码器实例\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                     ffn_num_hiddens, num_heads, num_layers, dropout)\n",
    "# 随机生成标记\n",
    "tokens = torch.randint(0, vocab_size, (2,8))\n",
    "# 创建段向量\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])  \n",
    "# 进行BERT编码\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "# 输出编码后的表示结果的形状\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Language Modeling\n",
    "class MaskLM(nn.Module):\n",
    "    \"\"\"The masked language model task of BERT.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),  # 全连接层：输入维度为num_inputs，输出维度为num_hiddens\n",
    "                                nn.ReLU(), # ReLU激活函数\n",
    "                                nn.LayerNorm(num_hiddens),  # LayerNorm层，归一化输入\n",
    "                                nn.Linear(num_hiddens, vocab_size)) # 全连接层：输入维度为num_hiddens，输出维度为vocab_size\n",
    "    \n",
    "    def forward(self, X, pred_positions):\n",
    "        # 预测位置数量\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        # 重塑预测位置张量为一维向量\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        # 批量大小\n",
    "        batch_size = X.shape[0]\n",
    "        # 创建批量索引向量\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        # 重复批量索引以匹配预测位置索引\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)  \n",
    "        # 从X中提取被掩盖的输入\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        # 重塑masked_X张量的形状\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        # 将masked_X传递给MLP网络，得到MLM任务的预测结果\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The forward inference of MaskLM\n",
    "# 创建MaskLM模型实例\n",
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "# 创建MLM任务的预测位置张量\n",
    "mlm_positions = torch.tensor([[1,5,2],[6,1,5]])\n",
    "# 输入编码后的文本和预测位置，得到MLM任务的预测结果\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "# 打印MLM任务的预测结果的形状\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建MLM任务的目标张量\n",
    "mlm_Y = torch.tensor([[7,8,9],[6,1,5]])\n",
    "# 创建交叉熵损失函数实例\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "# 计算MLM任务的损失\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "# 打印MLM任务的损失的形状\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Sentence Prediction\n",
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"The next sentence prediction task of BERT.\"\"\"\n",
    "    def __init__(self, num_inputs, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        # 全连接层用于预测下一句的概率\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # 输出下一句的预测结果\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The forward inference of an NextSentencePred\n",
    "# 将encoded_X展平为二维张量\n",
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "# 创建NextSentencePred实例，输入大小为encoded_X的最后一维大小\n",
    "nsp = NextSentencePred(encoded_X.shape[-1])\n",
    "# 使用nsp对encoded_X进行前向传播，得到下一句预测的结果\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "# 打印下一句预测结果的形状\n",
    "nsp_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建下一句预测的标签张量\n",
    "nsp_y = torch.tensor([0,1])\n",
    "# 计算下一句预测的损失\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "# 打印下一句预测损失的形状\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting All Things Together\n",
    "class BERTModel(nn.Module):\n",
    "    \"\"\"The BERT model.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                max_len=1000, key_size=768, mlm_in_features=768,\n",
    "                nsp_in_features=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        # BERT编码器\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,\n",
    "                                  ffn_num,input, ffn_num_hiddens, num_heads, num_layers,\n",
    "                                  dropout, max_len=max_len, key_size=key_size)\n",
    "        # 隐藏层\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens), nn.Tanh())   \n",
    "        # 掩码语言模型\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        # 下一句预测模型\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "        \n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        # 使用编码器对输入进行编码\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            # 如果传入了pred_positions参数，则调用掩码语言模型进行预测\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # 将encoded_X的第一个位置的隐藏表示通过隐藏层进行转换\n",
    "        # 使用下一句预测模型进行预测\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Pre-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The WikiText-2 dataset\n",
    "# 将WikiText-2数据集添加到d2l的数据集中心\n",
    "d2l.DATA_HUB['wikitext-2'] = ('https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "                             'wikitext-2-v1.zip','3c914d17d80b1459be871a5039ac23e752a53cbe')   \n",
    "\n",
    "def _read_wiki(data_dir):\n",
    "    # 读取WikiText-2数据集的训练集文件\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        # 逐行读取文件内容\n",
    "        lines = f.readlines()\n",
    "    # 将每行内容按句号分割成段落，并转换为小写\n",
    "    paragraphs = [line.strip().lower().split(' . ')\n",
    "                 for line in lines if len(line.split(' . ')) >= 2]\n",
    "    # 随机打乱段落的顺序\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the Next Sentence Prediction Task\n",
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    # 随机决定两个句子是否是下一个句子关系\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # 从随机选择的段落中选择一个句子作为下一个句子\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next\n",
    "\n",
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        # 获取当前句子和下一个句子以及它们之间的关系\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # 如果两个句子加上特殊标记的长度超过了最大长度，则跳过该句对\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        # 获取句子的token和segment表示\n",
    "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the Masked Language Modeling Task\n",
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    # 生成用于MLM任务的输入tokens，同时返回预测位置和标签\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    pred_positions_and_labels = []\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        # 如果已经预测了足够数量的位置，则结束\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        # 随机决定当前位置是否进行mask操作\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 随机决定是替换为当前token还是随机选择一个token作为替换\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            else:\n",
    "                masked_token = random.randint(0, len(vocab) - 1)\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))  \n",
    "    return mlm_input_tokens, pred_positions_and_labels\n",
    "\n",
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    candidate_pred_positions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 跳过特殊标记的位置\n",
    "        if token in ['<cls>','<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))    \n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
    "        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "    # 按照预测位置的顺序进行排序\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x:x[0])   \n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the special \"<mask>\" tokens to the inputs\n",
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    # 计算最大的预测位置数量\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    all_token_ids, all_segments, valid_lens, = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "    # 遍历每个样本\n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "        # 将输入 token 填充到指定长度，并转换为张量\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (max_len - len(token_ids)), dtype=torch.long))     \n",
    "        # 将输入 segment 填充到指定长度，并转换为张量\n",
    "        all_segments.append(torch.tensor(segments + [0] * (max_len - len(segments)), dtype=torch.long))\n",
    "        # 记录有效长度，即实际 token 的数量\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "        # 将预测位置填充到最大预测位置数量，并转换为张量\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n",
    "            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "        # 将 MLM 权重填充到最大预测位置数量，并转换为张量\n",
    "        all_mlm_weights.append(torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (max_num_mlm_preds - len(pred_positions)),\n",
    "                                           dtype = torch.float32))\n",
    "        # 将 MLM 预测标签填充到最大预测位置数量，并转换为张量\n",
    "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)),\n",
    "                                           dtype=torch.long))\n",
    "        # 记录 Next Sentence Prediction 的标签\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "    # 返回所有填充后的输入数据\n",
    "    return (all_token_ids, all_segments, valid_lens, all_pred_positions, \n",
    "            all_mlm_weights, all_mlm_labels, nsp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The WikiText-2 dataset for pretraining BERT\n",
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # 对段落进行分词，并构建词汇表\n",
    "        paragraphs = [d2l.tokenize(\n",
    "            paragraph, token='word') for paragraph in paragraphs]\n",
    "        sentences = [sentence for paragraph in paragraphs \n",
    "                     for sentence in paragraph]\n",
    "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens =[\n",
    "            '<pad>', '<mask>', '<cls>', '<seq>'])\n",
    "        examples = []\n",
    "        # 遍历每个段落，生成 Next Sentence Prediction 任务的数据\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(\n",
    "                paragraph, paragraphs, self.vocab, max_len))\n",
    "        # 遍历每个样本，生成 Masked Language Modeling 任务的数据，并对输入进行填充\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n",
    "                    + (segments, is_next))\n",
    "                   for tokens, segments, is_next in examples]\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "        self.all_pred_positions, self.all_mlm_weights,\n",
    "        self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(examples, max_len, self.vocab)   \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # 返回指定索引的数据\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "               self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "               self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
    "               self.nsp_labels[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 返回数据集的长度\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and WikiText-2 dataset and generate pretraining examples\n",
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"Load the WikiText-2 dataset. \"\"\"\n",
    "    # 获取用于加载数据的工作进程数\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    # 下载并解压 WikiText-2 数据集\n",
    "    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "    # 读取 WikiText-2 数据集中的段落\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    # 构建 WikiText-2 数据集对象\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    # 创建用于训练的数据迭代器\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                             shuffle=True, num_workers=0)  \n",
    "    # 返回训练数据迭代器和词汇表\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ..\\data\\wikitext-2-v1.zip from https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip...\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [29], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m batch_size, max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 加载 WikiText-2 数据集的训练数据迭代器和词汇表\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m train_iter, vocab \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_wiki\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 遍历训练数据迭代器，获取一个小批量的 BERT 预训练样本，并打印各项数据的形状\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n\u001b[0;32m      7\u001b[0m     mlm_Y, nsp_y) \u001b[38;5;129;01min\u001b[39;00m train_iter:\n",
      "Cell \u001b[1;32mIn [27], line 7\u001b[0m, in \u001b[0;36mload_data_wiki\u001b[1;34m(batch_size, max_len)\u001b[0m\n\u001b[0;32m      5\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mget_dataloader_workers()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 下载并解压 WikiText-2 数据集\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[43md2l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwikitext-2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwikitext-2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 读取 WikiText-2 数据集中的段落\u001b[39;00m\n\u001b[0;32m      9\u001b[0m paragraphs \u001b[38;5;241m=\u001b[39m _read_wiki(data_dir)\n",
      "File \u001b[1;32md:\\Users\\pc\\anaconda\\envs\\pytorch\\lib\\site-packages\\d2l\\torch.py:402\u001b[0m, in \u001b[0;36mdownload_extract\u001b[1;34m(name, folder)\u001b[0m\n\u001b[0;32m    400\u001b[0m data_dir, ext \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(fname)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 402\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ext \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gz\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    404\u001b[0m     fp \u001b[38;5;241m=\u001b[39m tarfile\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\pc\\anaconda\\envs\\pytorch\\lib\\zipfile.py:1267\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1267\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1269\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\pc\\anaconda\\envs\\pytorch\\lib\\zipfile.py:1334\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[1;32m-> 1334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1336\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "# Print out the shapes of a minibatch of BERT pretraining examples\n",
    "batch_size, max_len = 512, 64\n",
    "# 加载 WikiText-2 数据集的训练数据迭代器和词汇表\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "# 遍历训练数据迭代器，获取一个小批量的 BERT 预训练样本，并打印各项数据的形状\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
    "    mlm_Y, nsp_y) in train_iter:\n",
    "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
    "         pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
    "         nsp_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 打印词汇表的大小，即词汇表中不重复词汇的数量。\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mvocab\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# 打印词汇表的大小，即词汇表中不重复词汇的数量。\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ..\\data\\wikitext-2-v1.zip from https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip...\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m batch_size, max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 调用 load_data_wiki 函数加载 WikiText-2 数据集，并返回训练数据迭代器 train_iter 和词汇表 vocab\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m train_iter, vocab \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_wiki\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [19], line 7\u001b[0m, in \u001b[0;36mload_data_wiki\u001b[1;34m(batch_size, max_len)\u001b[0m\n\u001b[0;32m      5\u001b[0m num_workers \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mget_dataloader_workers()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 下载并解压 WikiText-2 数据集\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[43md2l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwikitext-2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwikitext-2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 读取 WikiText-2 数据集中的段落\u001b[39;00m\n\u001b[0;32m      9\u001b[0m paragraphs \u001b[38;5;241m=\u001b[39m _read_wiki(data_dir)\n",
      "File \u001b[1;32md:\\Users\\pc\\anaconda\\envs\\pytorch\\lib\\site-packages\\d2l\\torch.py:402\u001b[0m, in \u001b[0;36mdownload_extract\u001b[1;34m(name, folder)\u001b[0m\n\u001b[0;32m    400\u001b[0m data_dir, ext \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(fname)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 402\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ext \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gz\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    404\u001b[0m     fp \u001b[38;5;241m=\u001b[39m tarfile\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\pc\\anaconda\\envs\\pytorch\\lib\\zipfile.py:1267\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1267\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1269\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\pc\\anaconda\\envs\\pytorch\\lib\\zipfile.py:1334\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[1;32m-> 1334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1336\u001b[0m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "# 设置了训练数据的批量大小 batch_size 和最大长度 max_len。\n",
    "batch_size, max_len = 512, 64\n",
    "# 调用 load_data_wiki 函数加载 WikiText-2 数据集，并返回训练数据迭代器 train_iter 和词汇表 vocab\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
