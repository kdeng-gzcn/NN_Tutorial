{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis\n",
    "in 99% case, we use train accuracy, test accuracy, train loss, test loss for model metrics.\n",
    "\n",
    "the point is:\n",
    "1. how long will we train on the model\n",
    "2. at what point should we stop training\n",
    "3. which model is better on doing the same task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model analysis, we need to rewrite our train function to have better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from dataset import DogsVSCats\n",
    "\n",
    "# a trick\n",
    "REBUILD_DATA = False\n",
    "\n",
    "if REBUILD_DATA: # never run this again!\n",
    "    dogsvscats = DogsVSCats()\n",
    "    dogsvscats.make_training_data()\n",
    "\n",
    "training_data = np.load('training_data.npy', allow_pickle=True) # ???\n",
    "\n",
    "# reshape our data in nn\n",
    "X = torch.Tensor(np.array([i[0] for i in training_data])).view(-1, 50, 50)\n",
    "X = X / 255 # each pixel is in range [0, 255], this line is to scale the data\n",
    "\n",
    "y = torch.Tensor(np.array([i[1] for i in training_data]))\n",
    "\n",
    "valid_perc = 0.1\n",
    "valid_size = int(len(X) * valid_perc)\n",
    "valid_size\n",
    "\n",
    "train_X = X[:-valid_size]\n",
    "train_y = y[:-valid_size]\n",
    "\n",
    "test_X = X[-valid_size:]\n",
    "test_y = y[-valid_size:]\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define conv layer\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, # our image is 1 * 50 *50\n",
    "            out_channels=32,\n",
    "            kernel_size=5\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        # define pool layer\n",
    "        self.pool1 = nn.MaxPool2d(\n",
    "            kernel_size=(2, 2)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d((2, 2))\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        # define fc layer, but need to know the shape\n",
    "        value = 512\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=value, \n",
    "            out_features=512\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # a standard CNN structure is input -> (conv layer -> active func -> pool layer) -> next structure \n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "\n",
    "        # for first time, make sure the dim of x\n",
    "        x = x.flatten(start_dim=1)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # after feature extraction, use fc to do classification\n",
    "        # a standard FC structure is input -> (fc layer -> active func) -> next structure\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1) # make sure each row is a distribution (dim=1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train_net(net):\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001) # optimizer is supervising the net.params on GPU\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "    batch_size = 128\n",
    "    epochs = 10 # model analysis\n",
    "\n",
    "    # not using the API from torch.utils.data.DataLoader\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in tqdm(range(0, len(train_X), batch_size)):\n",
    "            # print(i, i + batch_size)\n",
    "            batch_X = train_X[i:i + batch_size].view(-1, 1, 50, 50).to(device) # claim the channel number\n",
    "            batch_y = train_y[i:i + batch_size].to(device) # no need to reshape?\n",
    "\n",
    "            # make sure the optimizer or net\n",
    "            net.zero_grad()\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch + 1} loss: ', loss.item())\n",
    "\n",
    "def test_net(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_X))): # batch = 1\n",
    "            y_ = torch.argmax(test_y[i]).to(device)\n",
    "            prob_hat = net(test_X[i].view([-1, 1, 50, 50]).to(device))[0] # reshape it into dim1\n",
    "            y_hat = torch.argmax(prob_hat)\n",
    "\n",
    "            if y_hat == y_:\n",
    "                correct += 1\n",
    "            \n",
    "            total += 1\n",
    "        \n",
    "        print('Accuracy: ', round(correct / total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_pass():\n",
    "    pass\n",
    "\n",
    "def test(size=32):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda:0')\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "train_net(net)\n",
    "test_net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gragph Visualization\n",
    "\n",
    "during the progress, we may want to track our loss, maybe in console, but visualization is better.\n",
    "\n",
    "- matplotlib\n",
    "- tensorboard\n",
    "- tensorboard X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOG our Model!!!\n",
    "import time\n",
    "\n",
    "Model_name = f'model-{int(time.time())}'\n",
    "\n",
    "net = Net().to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) # optimizer is supervising the net.params on GPU\n",
    "loss_function = nn.MSELoss()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
