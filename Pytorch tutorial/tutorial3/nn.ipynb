{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST(\n",
    "    root='',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "test = datasets.MNIST(\n",
    "    root='',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(\n",
    "    train,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "testset = torch.utils.data.DataLoader(\n",
    "    test,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # class\n",
    "import torch.nn.functional as F # function\n",
    "# usually we use class, but sometimes we want to write one function\n",
    "# so these 2 libraries are exchangable, they are similar\n",
    "# in F, it needs params, in nn, we initialize things\n",
    "\n",
    "# you would choose the library you want based on your case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# net is defined!!!\n",
    "# but note that the net wont learn, because it wont be scaled properly, thats because no activation function\n",
    "# it can work now, but not work as we want because no active function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # all neurons have the same active function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1) \n",
    "        # dim here is more likely to be axis= in pandas, it makes sure our y is distribution across numbers instead of batched\n",
    "        return x\n",
    "    \n",
    "# active function is mainly used to avoid number explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net() # net is object from class\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1995, 0.5587, 0.4291, 0.1885, 0.0928, 0.1360, 0.6073, 0.6862, 0.0127,\n",
       "         0.0463, 0.3490, 0.4747, 0.3312, 0.6269, 0.7363, 0.9269, 0.0083, 0.4318,\n",
       "         0.0078, 0.3723, 0.6368, 0.3397, 0.1478, 0.3539, 0.7702, 0.6180, 0.9213,\n",
       "         0.0522],\n",
       "        [0.3617, 0.5986, 0.4816, 0.6594, 0.0240, 0.7190, 0.1491, 0.4719, 0.9932,\n",
       "         0.6130, 0.0106, 0.1370, 0.3656, 0.2042, 0.6430, 0.5918, 0.8912, 0.8170,\n",
       "         0.5593, 0.8446, 0.9371, 0.8863, 0.3228, 0.5213, 0.2010, 0.5730, 0.5018,\n",
       "         0.0931],\n",
       "        [0.1573, 0.0876, 0.0186, 0.9652, 0.1592, 0.7263, 0.0763, 0.6054, 0.4710,\n",
       "         0.1383, 0.8548, 0.9172, 0.0116, 0.4027, 0.8768, 0.8957, 0.8236, 0.1566,\n",
       "         0.0445, 0.2059, 0.9920, 0.2170, 0.7839, 0.6346, 0.6259, 0.9942, 0.1023,\n",
       "         0.8088],\n",
       "        [0.9661, 0.0274, 0.3696, 0.2738, 0.3659, 0.0612, 0.4967, 0.3157, 0.4349,\n",
       "         0.8020, 0.5652, 0.8290, 0.0742, 0.9666, 0.7791, 0.0469, 0.3069, 0.5598,\n",
       "         0.7079, 0.8601, 0.4695, 0.3197, 0.1431, 0.7987, 0.4453, 0.0073, 0.2867,\n",
       "         0.2782],\n",
       "        [0.6499, 0.9203, 0.9618, 0.6959, 0.7469, 0.2328, 0.6195, 0.7827, 0.3705,\n",
       "         0.0502, 0.3422, 0.7014, 0.6622, 0.9250, 0.0601, 0.0527, 0.2074, 0.5698,\n",
       "         0.4361, 0.9029, 0.6156, 0.3130, 0.7069, 0.3503, 0.2858, 0.5733, 0.6489,\n",
       "         0.1922],\n",
       "        [0.3968, 0.8311, 0.1263, 0.8124, 0.9947, 0.7750, 0.7574, 0.6684, 0.4446,\n",
       "         0.3938, 0.7848, 0.1752, 0.5547, 0.1374, 0.9032, 0.3184, 0.9217, 0.7844,\n",
       "         0.2555, 0.3257, 0.9596, 0.4364, 0.8843, 0.2222, 0.1449, 0.5266, 0.8580,\n",
       "         0.3198],\n",
       "        [0.1512, 0.8387, 0.2541, 0.1640, 0.2413, 0.9285, 0.7109, 0.8438, 0.3497,\n",
       "         0.1825, 0.1828, 0.4853, 0.2889, 0.1514, 0.8346, 0.9181, 0.2574, 0.9865,\n",
       "         0.9216, 0.3607, 0.7468, 0.6181, 0.0735, 0.6677, 0.0500, 0.1345, 0.5709,\n",
       "         0.2794],\n",
       "        [0.4969, 0.6413, 0.2999, 0.4945, 0.4053, 0.2642, 0.5461, 0.2051, 0.8421,\n",
       "         0.6864, 0.8861, 0.1348, 0.9762, 0.7128, 0.9103, 0.5036, 0.4158, 0.7955,\n",
       "         0.6677, 0.0350, 0.5693, 0.2102, 0.1271, 0.2994, 0.5367, 0.0913, 0.9547,\n",
       "         0.8173],\n",
       "        [0.9250, 0.4397, 0.5911, 0.1640, 0.6998, 0.4312, 0.5764, 0.6694, 0.9495,\n",
       "         0.7235, 0.4759, 0.7963, 0.2423, 0.0175, 0.2414, 0.4563, 0.1980, 0.2066,\n",
       "         0.7027, 0.8793, 0.7412, 0.8264, 0.5981, 0.0457, 0.2860, 0.3302, 0.3282,\n",
       "         0.0783],\n",
       "        [0.3467, 0.3340, 0.9340, 0.7855, 0.6393, 0.5138, 0.4196, 0.7798, 0.4588,\n",
       "         0.4526, 0.9351, 0.0948, 0.2657, 0.8572, 0.3810, 0.5346, 0.8563, 0.2159,\n",
       "         0.6304, 0.1134, 0.7781, 0.2386, 0.2936, 0.2897, 0.7041, 0.0871, 0.6803,\n",
       "         0.3412],\n",
       "        [0.4036, 0.6293, 0.4504, 0.1038, 0.4451, 0.3239, 0.2373, 0.8248, 0.2017,\n",
       "         0.8348, 0.4471, 0.0435, 0.9345, 0.0974, 0.8858, 0.5527, 0.5337, 0.1074,\n",
       "         0.5546, 0.5622, 0.3020, 0.8768, 0.8231, 0.6882, 0.8891, 0.5951, 0.8546,\n",
       "         0.9151],\n",
       "        [0.8469, 0.5297, 0.7146, 0.3393, 0.2834, 0.4038, 0.0761, 0.7372, 0.1738,\n",
       "         0.5704, 0.1055, 0.9049, 0.0821, 0.5179, 0.2545, 0.5482, 0.8348, 0.3747,\n",
       "         0.0578, 0.2054, 0.4381, 0.5434, 0.3884, 0.5216, 0.7247, 0.7267, 0.5392,\n",
       "         0.9601],\n",
       "        [0.3078, 0.1715, 0.1953, 0.6897, 0.0553, 0.6016, 0.8671, 0.2590, 0.7281,\n",
       "         0.1331, 0.3996, 0.3074, 0.8238, 0.1932, 0.8501, 0.5364, 0.1245, 0.4939,\n",
       "         0.1437, 0.4849, 0.6256, 0.0595, 0.6656, 0.5893, 0.3852, 0.3426, 0.9435,\n",
       "         0.2876],\n",
       "        [0.3996, 0.2375, 0.5290, 0.6883, 0.4456, 0.4095, 0.6174, 0.9139, 0.9150,\n",
       "         0.3730, 0.0373, 0.4918, 0.0087, 0.1117, 0.9438, 0.9790, 0.0874, 0.1853,\n",
       "         0.0689, 0.9090, 0.9844, 0.9824, 0.1769, 0.8760, 0.3936, 0.9672, 0.1161,\n",
       "         0.8824],\n",
       "        [0.9616, 0.4421, 0.2755, 0.2796, 0.1576, 0.3043, 0.3460, 0.5768, 0.3889,\n",
       "         0.0921, 0.4827, 0.8212, 0.7327, 0.2119, 0.8226, 0.5209, 0.3674, 0.5381,\n",
       "         0.2373, 0.1971, 0.3869, 0.9345, 0.1455, 0.8691, 0.8206, 0.7688, 0.5785,\n",
       "         0.6757],\n",
       "        [0.6042, 0.6922, 0.8027, 0.0471, 0.0080, 0.7683, 0.2322, 0.5359, 0.5701,\n",
       "         0.4358, 0.7191, 0.5118, 0.6713, 0.3429, 0.0503, 0.9352, 0.6232, 0.0672,\n",
       "         0.3773, 0.9725, 0.3955, 0.6657, 0.9945, 0.9408, 0.6465, 0.9283, 0.3103,\n",
       "         0.5086],\n",
       "        [0.7895, 0.6158, 0.8661, 0.5213, 0.5208, 0.3232, 0.8374, 0.0844, 0.8595,\n",
       "         0.1644, 0.4738, 0.0021, 0.9237, 0.8708, 0.7958, 0.7103, 0.8501, 0.9784,\n",
       "         0.2269, 0.0213, 0.7051, 0.3661, 0.7791, 0.3759, 0.2450, 0.3368, 0.2112,\n",
       "         0.7440],\n",
       "        [0.4765, 0.0678, 0.1798, 0.1909, 0.1884, 0.6116, 0.6383, 0.8639, 0.1717,\n",
       "         0.2550, 0.1949, 0.3659, 0.2493, 0.7891, 0.8395, 0.1353, 0.1056, 0.8478,\n",
       "         0.8125, 0.4525, 0.5353, 0.3138, 0.2397, 0.3198, 0.5619, 0.8026, 0.0390,\n",
       "         0.6155],\n",
       "        [0.6497, 0.9205, 0.3191, 0.0951, 0.0741, 0.8830, 0.5230, 0.1799, 0.8919,\n",
       "         0.5737, 0.7735, 0.4287, 0.0847, 0.0743, 0.8923, 0.3170, 0.8781, 0.6862,\n",
       "         0.6226, 0.0998, 0.1432, 0.4437, 0.8890, 0.4346, 0.2488, 0.0815, 0.8366,\n",
       "         0.2571],\n",
       "        [0.0241, 0.7463, 0.3709, 0.1608, 0.0554, 0.9086, 0.5748, 0.1649, 0.1222,\n",
       "         0.4379, 0.1847, 0.9417, 0.0522, 0.7920, 0.7062, 0.8371, 0.7900, 0.8663,\n",
       "         0.9921, 0.5756, 0.5731, 0.7086, 0.1968, 0.1469, 0.5103, 0.7438, 0.8267,\n",
       "         0.7344],\n",
       "        [0.0040, 0.9726, 0.1638, 0.7155, 0.2880, 0.9784, 0.7628, 0.4471, 0.2827,\n",
       "         0.0822, 0.6886, 0.4499, 0.6927, 0.7120, 0.4914, 0.4193, 0.4493, 0.2370,\n",
       "         0.8346, 0.3081, 0.8921, 0.3520, 0.1344, 0.9509, 0.0215, 0.2212, 0.2278,\n",
       "         0.9285],\n",
       "        [0.5098, 0.5107, 0.9992, 0.3453, 0.7373, 0.5534, 0.6303, 0.9153, 0.9572,\n",
       "         0.1247, 0.3209, 0.5359, 0.3461, 0.9593, 0.2239, 0.7059, 0.6757, 0.4394,\n",
       "         0.9365, 0.7617, 0.2818, 0.0627, 0.6521, 0.1555, 0.8389, 0.9460, 0.8823,\n",
       "         0.7700],\n",
       "        [0.0376, 0.3294, 0.6136, 0.0541, 0.9522, 0.7336, 0.8367, 0.2414, 0.9744,\n",
       "         0.3177, 0.9300, 0.2345, 0.4089, 0.6340, 0.9296, 0.9828, 0.5101, 0.4605,\n",
       "         0.0220, 0.6645, 0.2183, 0.6177, 0.4633, 0.4684, 0.8024, 0.5006, 0.5842,\n",
       "         0.7053],\n",
       "        [0.9107, 0.1801, 0.5473, 0.6676, 0.2516, 0.0722, 0.3830, 0.8541, 0.2800,\n",
       "         0.1818, 0.7713, 0.6484, 0.7935, 0.9994, 0.4654, 0.2542, 0.9337, 0.3550,\n",
       "         0.3581, 0.6162, 0.4236, 0.3747, 0.7653, 0.9337, 0.9428, 0.1592, 0.3992,\n",
       "         0.8946],\n",
       "        [0.5689, 0.1448, 0.0332, 0.6070, 0.3529, 0.5059, 0.1272, 0.5574, 0.7134,\n",
       "         0.6169, 0.0692, 0.6442, 0.9710, 0.8400, 0.2956, 0.6021, 0.1776, 0.9510,\n",
       "         0.9637, 0.6642, 0.9712, 0.9386, 0.0598, 0.5018, 0.4293, 0.8942, 0.1737,\n",
       "         0.2861],\n",
       "        [0.4512, 0.0224, 0.9070, 0.4844, 0.3534, 0.7693, 0.2684, 0.8077, 0.0806,\n",
       "         0.1192, 0.6750, 0.9310, 0.7420, 0.1387, 0.8683, 0.5961, 0.8396, 0.8597,\n",
       "         0.1515, 0.3226, 0.3957, 0.3254, 0.0360, 0.7476, 0.0646, 0.3571, 0.9323,\n",
       "         0.0523],\n",
       "        [0.3696, 0.3046, 0.7752, 0.8785, 0.7375, 0.2874, 0.6711, 0.7856, 0.4408,\n",
       "         0.4566, 0.2810, 0.5627, 0.3095, 0.5714, 0.0557, 0.4539, 0.5266, 0.3388,\n",
       "         0.8129, 0.9042, 0.6130, 0.6746, 0.4967, 0.6939, 0.0057, 0.1705, 0.3550,\n",
       "         0.1036],\n",
       "        [0.5498, 0.0381, 0.4189, 0.7533, 0.2687, 0.0790, 0.4551, 0.5799, 0.8656,\n",
       "         0.1708, 0.6111, 0.0981, 0.3152, 0.3662, 0.9477, 0.4775, 0.2672, 0.9108,\n",
       "         0.5337, 0.9017, 0.1823, 0.6958, 0.3402, 0.8777, 0.4743, 0.5218, 0.4664,\n",
       "         0.0429]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to use our nn\n",
    "X = torch.rand([28, 28])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (28x28 and 784x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\pc\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [4], line 10\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# all neurons have the same active function\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))\n",
      "File \u001b[1;32md:\\Users\\pc\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Users\\pc\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x64)"
     ]
    }
   ],
   "source": [
    "output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3224, -2.2490, -2.3370, -2.4818, -2.2935, -2.2039, -2.3357, -2.2374,\n",
       "         -2.3684, -2.2269]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.view([-1, 28 * 28])\n",
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
